{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Exporting the Model\n",
    "In this notebook, you'll explore options for exporting a BERT checkpoint trained using PyTorch, to NVIDIA Triton Inference Server.\n",
    "\n",
    "**[1.1 Overview: Optimization and Performance](#1.1-Overview:-Optimization-and-Performance)<br>**\n",
    "**[1.2 Export a BERT Checkpoint](#1.2-Export-a-BERT-Checkpoint)<br>**\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [1.2.1 Triton Model Repository](#1.2.1-Triton-Model-Repository)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [1.2.2 TorchScript Export](#1.2.2-TorchScript-Export)<br>\n",
    "**[1.3 Test Our Export](#1.3-Test-Our-Export)<br>**\n",
    "**[1.4 Beyond TorchScript](#1.4-Beyond-TorchScript)<br>**\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [1.4.1 Exercise: Enable TensorRT Optimization](#1.4.1-Exercise:-Enable-TensorRT-Optimization)<br>\n",
    "**[1.5 Performance Comparison](#1.5-Performance-Comparison)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Overview: Optimization and Performance\n",
    "Optimization of the trained model will have a fairly dramatic impact on the inference performance, measured in bandwidth and latency. Even if the project requirements do not justify investing engineering effort into advanced techniques, such as knowledge distillation or pruning, a fair amount of model performance improvement can be achieved by using model optimization tools. The diagram below illustrates the difference in inference performance between a model deployed using non-optimized TensorFlow, the same model post-processed with TensorRT, and a model fully optimized with TensorRT. \n",
    "\n",
    "<img src=\"images/TFvTRT.jpg\" alt=\"Header\" style=\"width: 600px;\"/>\n",
    "\n",
    "Modern inference servers typically support substantially more than one model format to cater to a wider range of projects, tools, and preferences. Since in this class we are working with a BERT checkpoint trained using PyTorch, and we are deploying it with Triton Inference Server, we will focus on options for deploying PyTorch-based models. These include:\n",
    "   - PyTorch JIT / TorchScript\n",
    "   - ONNX runtime\n",
    "   - ONNX-TensorRT\n",
    "   - TensorRT\n",
    "    \n",
    "It's important to point out that Triton Server supports a much broader set of deployment mechanisms including:\n",
    "   - TensorFlow GraphDef\n",
    "   - TensorFlow saved model\n",
    "   - Caffe 2 exports\n",
    "   - Custom models (which can be any custom executable)\n",
    "\n",
    "In this section we will look at how to deploy a model using some of the deployment engines listed above and the impact each has on performance. We will also experiment with some of the key settings, namely the batch size and numerical precision (FP32 and FP16)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Export a BERT Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT model checkpoint we want to deploy, <code>bert_qa.pt</code>, should be located in your `data` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/bert_qa.pt\n"
     ]
    }
   ],
   "source": [
    "!ls data/*.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is a standard checkpoint of a BERT-Large network, fine-tuned on the [Stanford Question Answering Dataset (SQuAD)](https://arxiv.org/abs/1606.05250). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Scripts\n",
    "As we explore various deployment configurations, we'll repeat some steps over and over.  Therefore, we'll use some helper scripts to partially automate the process so that we can focus our attention on the configuration settings and results.  You can explore the code details yourself if you are curious:\n",
    "\n",
    "- [utilities/wait_for_triton_server.sh](utilities/wait_for_triton_server.sh): Check the \"live\" and \"ready\" status of the Triton server via the API\n",
    "- [deployer/deployer.py](deployer/deployer.py): Convert a checkpoint to a deployable model and export it\n",
    "- [uitlities/run_perf_client_local.sh](utilities/run_perf_client_local.sh): Measure performance with the [perf_client](https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/perf_client.html) application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Triton server has been deployed in a container and is available to us at host \"triton\" on port \"8000\". Run the next cell to to check for a \"200 OK\" HTTP response from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n"
     ]
    }
   ],
   "source": [
    "# Set the server hostname and check it - you should get a message that \"Triton Server is ready!\"\n",
    "tritonServerHostName = \"triton\"\n",
    "!./utilities/wait_for_triton_server.sh {tritonServerHostName}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1 Triton Model Repository\n",
    "When Triton Server is started, it is typically configured to observe a local or remote file system where models are hosted. The directory which is being observed is called a *model repository*. A typical command to start the Triton Server identifies the location of the model repository with an option:<br>\n",
    "```bash\n",
    "tritonserver --model-repository=\"/path/to/model/repository\"\n",
    "```\n",
    "\n",
    "The model repository needs to have the following layout:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "<model-repository-path>/\n",
    "  <model-name>/\n",
    "    [config.pbtxt]\n",
    "    [<output-labels-file> ...]\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    ...\n",
    "  <model-name>/\n",
    "    [config.pbtxt]\n",
    "    [<output-labels-file> ...]\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    ...\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab container is configured to use the <code>./model_repository</code> folder as the model repository, so any change within this folder will affect the behavior of Triton Server.<br/>\n",
    "\n",
    "In order to expose a new model to Triton you need to: <br/>\n",
    "   1. Create a new model folder in the model repository. The name of the folder needs to reflect the name of the service you will be exposing to your users/applications.<br/>\n",
    "   2. Within the model folder, create a <code>config.pbtxt</code> file that contains the basic serving configuration for the model<br/>\n",
    "   3. Also within the model folder, create at least one folder containing a copy of the model. The name of the folder reflects the version name of the model. You can create and host multiple versions of the same model.<br/>\n",
    "    \n",
    "Next, we'll walk through the process of exporting the model to Triton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2 TorchScript Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the lab we will:\n",
    "   - Convert the PyTorch checkpoint into [TorchScript](https://pytorch.org/docs/stable/jit.html#torchscript)\n",
    "   - Generate the Triton configuration file\n",
    "   - Deploy the created assets to our model repository\n",
    "Please execute the cells below. Since we are loading a PyTorch checkpoint and converting it into TorchScript, it might take a minute or two to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-torchscript\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-torchscript in format pytorch_libtorch\n",
      "/opt/conda/lib/python3.6/site-packages/torch/jit/_recursive.py:160: UserWarning: 'bias' was found in ScriptModule constants,  but it is a non-constant parameter. Consider removing it.\n",
      "  \" but it is a non-constant {}. Consider removing it.\".format(name, hint))\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  8.344650268554688e-06\n",
      "\n",
      "average L_inf error over output tensors:  8.106231689453125e-06\n",
      "variance of L_inf error over output tensors:  7.579122514774402e-14\n",
      "stddev of L_inf error over output tensors:  2.7530206164819037e-07\n",
      "\n",
      "time of error check of native model:  0.6739401817321777 seconds\n",
      "time of error check of ts model:  1.907928705215454 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --ts-script \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint \"/dli/task/data/bert_qa.pt\" \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `deployer.py` script loads the `bert_qa.pt` checkpoint, deploys it in `ts-script` format into a folder called `bertQA-torchscript`, and marks it as version `1`. We will discuss some of the more advanced settings later. For now, let's inspect the files generated by the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 3 root root 4096 Sep 14 13:42 .\n",
      "drwxr-xr-x 3 root root 4096 Sep 14 13:42 ..\n",
      "drwxr-xr-x 2 root root 4096 Sep 14 13:42 1\n",
      "-rw-r--r-- 1 root root  568 Sep 14 13:42 config.pbtxt\n",
      "total 1309292\n",
      "drwxr-xr-x 2 root root       4096 Sep 14 13:42 .\n",
      "drwxr-xr-x 3 root root       4096 Sep 14 13:42 ..\n",
      "-rw-r--r-- 1 root root 1340706048 Sep 14 13:42 model.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./candidatemodels/bertQA-torchscript/\n",
    "!ls -al ./candidatemodels/bertQA-torchscript/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the script exported the model into the TorchScript format and saved it as `model.pt`. It also generated the `config.pbtxt` file. <br> \n",
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"bertQA-torchscript\"\n",
      "platform: \"pytorch_libtorch\"\n",
      "max_batch_size: 8\n",
      "input [\n",
      "{\n",
      "    name: \"input__0\"\n",
      "    data_type: TYPE_INT64\n",
      "    dims: [384]\n",
      "},\n",
      "{\n",
      "    name: \"input__1\"\n",
      "    data_type: TYPE_INT64\n",
      "    dims: [384]\n",
      "},\n",
      "{\n",
      "    name: \"input__2\"\n",
      "    data_type: TYPE_INT64\n",
      "    dims: [384]\n",
      "}\n",
      "]\n",
      "output [\n",
      "{\n",
      "    name: \"output__0\"\n",
      "    data_type: TYPE_FP32\n",
      "    dims: [384]\n",
      "}, \n",
      "{\n",
      "    name: \"output__1\"\n",
      "    data_type: TYPE_FP32\n",
      "    dims: [384]\n",
      "}\n",
      "]\n",
      "optimization {\n",
      "  cuda {\n",
      "    graphs: 0\n",
      "  }\n",
      "}\n",
      "instance_group [\n",
      "    {\n",
      "        count: 1\n",
      "        kind: KIND_GPU\n",
      "        gpus: [ 0 ]\n",
      "    }\n",
      "]"
     ]
    }
   ],
   "source": [
    "!cat ./candidatemodels/bertQA-torchscript/config.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file is fairly simple and defines:\n",
    "   - Name of the model\n",
    "   - Type of platform to be used for inference; in this case `pytorch_libtorch`\n",
    "   - Input and output dimensions used by the network\n",
    "   - Optimizations used; in this case GPU and the default TorchScript optimization \n",
    "   - Instance group configuration; in this case instance group count is set to one, meaning that only one copy of the model will be held in GPU memory (GPU 0 is being used).\n",
    "    \n",
    "To deploy the model, move the folder to the Triton model repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-torchscript model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations!  You have successfully deployed your first model to Triton Inference Server!\n",
    "\n",
    "We'll come back to discuss the detailed configuration later, but for now let's see how our model is performing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1.3 Test Our Export\n",
    "Execute the cells below to start an inference process and make a simple measurement of inference performance. First, we'll set up some configuration. `maxConcurrency` is set to two, meaning that the stress test will be executed twice. The first run will use just a single thread and the second one will use two threads to query the server. Without turning on the concurrent model execution or dynamic batching features, what do you think will be the impact on performance of running two processes querying the server? Do you think:<br/>\n",
    "- Bandwidth will increase or decrease?<br/>\n",
    "- Latency will increase or decrease?<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelVersion=\"1\"\n",
    "precision=\"fp32\"\n",
    "batchSize=\"1\"\n",
    "maxLatency=\"500\"\n",
    "maxClientThreads=\"10\"\n",
    "maxConcurrency=\"2\"\n",
    "dockerBridge=\"host\"\n",
    "resultsFolderName=\"1\"\n",
    "profilingData=\"utilities/profiling_data_int64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 2 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 25 infer/sec. Avg latency: 43623 usec (std 80806 usec)\n",
      "  Pass [2] throughput: 29.3333 infer/sec. Avg latency: 34193 usec (std 60 usec)\n",
      "  Pass [3] throughput: 29 infer/sec. Avg latency: 34221 usec (std 81 usec)\n",
      "  Pass [4] throughput: 29.3333 infer/sec. Avg latency: 34206 usec (std 90 usec)\n",
      "  Client: \n",
      "    Request count: 88\n",
      "    Throughput: 29.3333 infer/sec\n",
      "    Avg latency: 34206 usec (standard deviation 90 usec)\n",
      "    p50 latency: 34192 usec\n",
      "    p90 latency: 34275 usec\n",
      "    p95 latency: 34340 usec\n",
      "    p99 latency: 34352 usec\n",
      "    Avg HTTP time: 34191 usec (send 8 usec + response wait 34181 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 105\n",
      "    Execution count: 105\n",
      "    Successful request count: 105\n",
      "    Avg request latency: 33828 usec (overhead 4 usec + queue 796 usec + compute input 60 usec + compute infer 32465 usec + compute output 503 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 29.3333 infer/sec. Avg latency: 68406 usec (std 99 usec)\n",
      "  Pass [2] throughput: 26 infer/sec. Avg latency: 76198 usec (std 11211 usec)\n",
      "  Pass [3] throughput: 26.6667 infer/sec. Avg latency: 75015 usec (std 3331 usec)\n",
      "  Client: \n",
      "    Request count: 80\n",
      "    Throughput: 26.6667 infer/sec\n",
      "    Avg latency: 75015 usec (standard deviation 3331 usec)\n",
      "    p50 latency: 76440 usec\n",
      "    p90 latency: 77048 usec\n",
      "    p95 latency: 77414 usec\n",
      "    p99 latency: 78901 usec\n",
      "    Avg HTTP time: 74439 usec (send 7 usec + response wait 74430 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 97\n",
      "    Execution count: 97\n",
      "    Successful request count: 97\n",
      "    Avg request latency: 74062 usec (overhead 3 usec + queue 38102 usec + compute input 62 usec + compute infer 35604 usec + compute output 291 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 29.3333 infer/sec, latency 34206 usec\n",
      "Concurrency: 2, throughput: 26.6667 infer/sec, latency 75015 usec\n"
     ]
    }
   ],
   "source": [
    "!./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went okay you should have been presented with output similar to the following example result, showing the inference performance across two different configurations.<br/>\n",
    "<img src=\"images/InferenceJob1.png\" alt=\"Example output of inference job 1\" style=\"width: 1200px;\"/>\n",
    "\n",
    "If you happened to get \"error: failed to get model metatdata\", try running the cell again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Beyond TorchScript\n",
    "\n",
    "Let's investigate a different route for model deployment onto Triton, namely <a href=\"https://onnx.ai\">Open Neural Network Exchange (ONNX)</a>. ONNX is an open format for representation and exchange of neural network models. It defines a common set of operators that are used to build common models, as well as a file format for exchanging them. The advantage of ONNX is that it is relatively widely adopted and can be used to exchange models between <a href=\"https://onnx.ai/supported-tools.html\">a wide range of deep learning tools</a>, such as deep learning frameworks or deployment tools. This also includes TensorRT, which can consume ONNX models. </br>\n",
    "\n",
    "As before, start by exporting the model, but this time using the ONNX format. We will take advantage of the export tool that we used earlier, but change the export format from <code>ts-script</code> to <code>onnx</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx in format onnxruntime_onnx\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1336539973\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.00022935867309570312\n",
      "\n",
      "average L_inf error over output tensors:  0.0001423954963684082\n",
      "variance of L_inf error over output tensors:  6.657553323445124e-09\n",
      "stddev of L_inf error over output tensors:  8.159383140559784e-05\n",
      "\n",
      "time of error check of native model:  0.4087388515472412 seconds\n",
      "time of error check of onnx model:  16.82509207725525 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/docs/serialization.md\">TorchScript serialization format</a>, the <a href=\"https://onnx.ai/get-started.html\">ONNX format</a> can be inspected quite easily (and parts are human readable). Lets have a look at the assets our export has generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 3 root root 4096 Sep 14 13:43 .\n",
      "drwxr-xr-x 3 root root 4096 Sep 14 13:43 ..\n",
      "drwxr-xr-x 2 root root 4096 Sep 14 13:43 1\n",
      "-rw-r--r-- 1 root root  561 Sep 14 13:43 config.pbtxt\n",
      "total 1305224\n",
      "drwxr-xr-x 2 root root       4096 Sep 14 13:43 .\n",
      "drwxr-xr-x 3 root root       4096 Sep 14 13:43 ..\n",
      "-rw-r--r-- 1 root root 1336539973 Sep 14 13:43 model.onnx\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./candidatemodels/bertQA-onnx/\n",
    "!ls -al ./candidatemodels/bertQA-onnx/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we have a configuration file as well as a model, this time stored in ONNX format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a couple of options for executing the ONNX-based export in Triton:\n",
    "- We can take advantage of ONNX runtime </br>\n",
    "- We can ask TensorRT to parse the ONNX assets in order to generate a TensorRT engine to use instead </br>\n",
    "\n",
    "We'll try both approaches and look at the impact this has on inference performance. In order to deploy the current ONNX model, move it to the model repository..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and run our stress testing code across 10 different levels of concurrency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 48 infer/sec. Avg latency: 163305 usec (std 3360 usec)\n",
      "  Pass [2] throughput: 50.6667 infer/sec. Avg latency: 163258 usec (std 219 usec)\n",
      "  Pass [3] throughput: 50.6667 infer/sec. Avg latency: 163666 usec (std 376 usec)\n",
      "  Client: \n",
      "    Request count: 19\n",
      "    Throughput: 50.6667 infer/sec\n",
      "    Avg latency: 163666 usec (standard deviation 376 usec)\n",
      "    p50 latency: 163638 usec\n",
      "    p90 latency: 164086 usec\n",
      "    p95 latency: 164088 usec\n",
      "    p99 latency: 164284 usec\n",
      "    Avg HTTP time: 163603 usec (send 21 usec + response wait 163580 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 176\n",
      "    Execution count: 22\n",
      "    Successful request count: 22\n",
      "    Avg request latency: 163034 usec (overhead 2 usec + queue 33 usec + compute input 30 usec + compute infer 162940 usec + compute output 29 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 48 infer/sec. Avg latency: 325863 usec (std 607 usec)\n",
      "  Pass [2] throughput: 48 infer/sec. Avg latency: 326408 usec (std 1819 usec)\n",
      "  Pass [3] throughput: 48 infer/sec. Avg latency: 327159 usec (std 1189 usec)\n",
      "  Client: \n",
      "    Request count: 18\n",
      "    Throughput: 48 infer/sec\n",
      "    Avg latency: 327159 usec (standard deviation 1189 usec)\n",
      "    p50 latency: 327003 usec\n",
      "    p90 latency: 328472 usec\n",
      "    p95 latency: 329399 usec\n",
      "    p99 latency: 329882 usec\n",
      "    Avg HTTP time: 327456 usec (send 18 usec + response wait 327436 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 176\n",
      "    Execution count: 22\n",
      "    Successful request count: 22\n",
      "    Avg request latency: 326936 usec (overhead 3 usec + queue 163326 usec + compute input 25 usec + compute infer 163560 usec + compute output 22 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 48 infer/sec. Avg latency: 481967 usec (std 37314 usec)\n",
      "  Pass [2] throughput: 50.6667 infer/sec. Avg latency: 490755 usec (std 1515 usec)\n",
      "  Pass [3] throughput: 50.6667 infer/sec. Avg latency: 491638 usec (std 1615 usec)\n",
      "  Client: \n",
      "    Request count: 19\n",
      "    Throughput: 50.6667 infer/sec\n",
      "    Avg latency: 491638 usec (standard deviation 1615 usec)\n",
      "    p50 latency: 491381 usec\n",
      "    p90 latency: 493765 usec\n",
      "    p95 latency: 494093 usec\n",
      "    p99 latency: 495613 usec\n",
      "    Avg HTTP time: 491698 usec (send 20 usec + response wait 491676 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 176\n",
      "    Execution count: 22\n",
      "    Successful request count: 22\n",
      "    Avg request latency: 491179 usec (overhead 3 usec + queue 327390 usec + compute input 21 usec + compute infer 163747 usec + compute output 18 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 48 infer/sec. Avg latency: 628897 usec (std 61260 usec)\n",
      "  Pass [2] throughput: 48 infer/sec. Avg latency: 656143 usec (std 1502 usec)\n",
      "  Pass [3] throughput: 48 infer/sec. Avg latency: 655788 usec (std 1475 usec)\n",
      "  Client: \n",
      "    Request count: 18\n",
      "    Throughput: 48 infer/sec\n",
      "    Avg latency: 655788 usec (standard deviation 1475 usec)\n",
      "    p50 latency: 656863 usec\n",
      "    p90 latency: 657203 usec\n",
      "    p95 latency: 657304 usec\n",
      "    p99 latency: 657320 usec\n",
      "    Avg HTTP time: 656337 usec (send 20 usec + response wait 656315 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 176\n",
      "    Execution count: 22\n",
      "    Successful request count: 22\n",
      "    Avg request latency: 655710 usec (overhead 4 usec + queue 491745 usec + compute input 25 usec + compute infer 163915 usec + compute output 21 usec)\n",
      "\n",
      "Measured latency went over the set limit of 500 msec. \n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 50.6667 infer/sec, latency 163666 usec\n",
      "Concurrency: 2, throughput: 48 infer/sec, latency 327159 usec\n",
      "Concurrency: 3, throughput: 50.6667 infer/sec, latency 491638 usec\n",
      "Concurrency: 4, throughput: 48 infer/sec, latency 655788 usec\n"
     ]
    }
   ],
   "source": [
    "modelName = \"bertQA-onnx\"\n",
    "maxConcurrency = \"10\"\n",
    "batchSize = \"8\"\n",
    "print(\"Running: \"+modelName)\n",
    "!bash ./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the results. Did we manage to run our benchmark at all 10 concurrency levels (or did the benchmark time out earlier)? What happened to the request latency in relation to the 500 ms time limit we configured?</br>\n",
    "\n",
    "Now let's export the ONNX model again, so that we can configure it for TensorRT execution.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-trt-fp16\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-trt-fp16 in format onnxruntime_onnx\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1336539973\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.00022935867309570312\n",
      "\n",
      "average L_inf error over output tensors:  0.0001423954963684082\n",
      "variance of L_inf error over output tensors:  6.657553323445124e-09\n",
      "stddev of L_inf error over output tensors:  8.159383140559784e-05\n",
      "\n",
      "time of error check of native model:  0.4407467842102051 seconds\n",
      "time of error check of onnx model:  16.894017934799194 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again the above command should have generated the ONNX export as well as a configuration file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 3 root root 4096 Sep 14 13:48 .\n",
      "drwxr-xr-x 3 root root 4096 Sep 14 13:48 ..\n",
      "drwxr-xr-x 2 root root 4096 Sep 14 13:48 1\n",
      "-rw-r--r-- 1 root root  570 Sep 14 13:48 config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./candidatemodels/bertQA-onnx-trt-fp16/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.1 Exercise: Enable TensorRT Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to enable TensorRT, we need to add an additional section to the \"config.pbtxt\" configuration file. In particular, we need to add an additional segment to the <code>optimization</code> section:\n",
    "\n",
    "```text\n",
    "optimization {\n",
    "   execution_accelerators {\n",
    "      gpu_execution_accelerator : [ {\n",
    "         name : \"tensorrt\"\n",
    "         parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "      }]\n",
    "   }\n",
    "cuda { graphs: 0 }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Exercise Steps:\n",
    "1. Modify [config.pbtxt](candidatemodels/bertQA-onnx-trt-fp16/config.pbtxt) to enable TensorRT. Feel free to look at the [solution](solutions/ex-1-4-1_config.pbtxt) as needed.\n",
    "2. Once you have saved your changes (Main menu: File -> Save File), move the folder to the model repository using the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx-trt-fp16 model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Execute our profiling tool in the next cell and investigate the impact on performance. This could take a while to start, as we are waiting for the server to migrate the model to TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-fp16\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "..............................................Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 290.667 infer/sec. Avg latency: 27620 usec (std 322 usec)\n",
      "  Pass [2] throughput: 290.667 infer/sec. Avg latency: 27632 usec (std 148 usec)\n",
      "  Pass [3] throughput: 290.667 infer/sec. Avg latency: 27667 usec (std 151 usec)\n",
      "  Client: \n",
      "    Request count: 109\n",
      "    Throughput: 290.667 infer/sec\n",
      "    Avg latency: 27667 usec (standard deviation 151 usec)\n",
      "    p50 latency: 27610 usec\n",
      "    p90 latency: 27888 usec\n",
      "    p95 latency: 27928 usec\n",
      "    p99 latency: 27994 usec\n",
      "    Avg HTTP time: 27651 usec (send 9 usec + response wait 27641 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1040\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 27303 usec (overhead 3 usec + queue 29 usec + compute input 17 usec + compute infer 27240 usec + compute output 14 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 293.333 infer/sec. Avg latency: 54849 usec (std 250 usec)\n",
      "  Pass [2] throughput: 293.333 infer/sec. Avg latency: 54883 usec (std 232 usec)\n",
      "  Pass [3] throughput: 290.667 infer/sec. Avg latency: 54912 usec (std 263 usec)\n",
      "  Client: \n",
      "    Request count: 109\n",
      "    Throughput: 290.667 infer/sec\n",
      "    Avg latency: 54912 usec (standard deviation 263 usec)\n",
      "    p50 latency: 54917 usec\n",
      "    p90 latency: 55213 usec\n",
      "    p95 latency: 55467 usec\n",
      "    p99 latency: 55691 usec\n",
      "    Avg HTTP time: 54895 usec (send 11 usec + response wait 54883 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1048\n",
      "    Execution count: 131\n",
      "    Successful request count: 131\n",
      "    Avg request latency: 54499 usec (overhead 3 usec + queue 27135 usec + compute input 16 usec + compute infer 27331 usec + compute output 14 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 290.667 infer/sec. Avg latency: 82311 usec (std 369 usec)\n",
      "  Pass [2] throughput: 293.333 infer/sec. Avg latency: 82346 usec (std 368 usec)\n",
      "  Pass [3] throughput: 293.333 infer/sec. Avg latency: 82439 usec (std 382 usec)\n",
      "  Client: \n",
      "    Request count: 110\n",
      "    Throughput: 293.333 infer/sec\n",
      "    Avg latency: 82439 usec (standard deviation 382 usec)\n",
      "    p50 latency: 82453 usec\n",
      "    p90 latency: 82874 usec\n",
      "    p95 latency: 83169 usec\n",
      "    p99 latency: 83372 usec\n",
      "    Avg HTTP time: 82416 usec (send 10 usec + response wait 82405 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1048\n",
      "    Execution count: 131\n",
      "    Successful request count: 131\n",
      "    Avg request latency: 82016 usec (overhead 3 usec + queue 54624 usec + compute input 16 usec + compute infer 27359 usec + compute output 14 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 293.333 infer/sec. Avg latency: 109870 usec (std 270 usec)\n",
      "  Pass [2] throughput: 290.667 infer/sec. Avg latency: 109848 usec (std 335 usec)\n",
      "  Pass [3] throughput: 290.667 infer/sec. Avg latency: 109858 usec (std 262 usec)\n",
      "  Client: \n",
      "    Request count: 109\n",
      "    Throughput: 290.667 infer/sec\n",
      "    Avg latency: 109858 usec (standard deviation 262 usec)\n",
      "    p50 latency: 109886 usec\n",
      "    p90 latency: 110183 usec\n",
      "    p95 latency: 110248 usec\n",
      "    p99 latency: 110334 usec\n",
      "    Avg HTTP time: 109866 usec (send 13 usec + response wait 109852 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1048\n",
      "    Execution count: 131\n",
      "    Successful request count: 131\n",
      "    Avg request latency: 109448 usec (overhead 3 usec + queue 82062 usec + compute input 17 usec + compute infer 27353 usec + compute output 13 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 290.667 infer/sec. Avg latency: 137406 usec (std 421 usec)\n",
      "  Pass [2] throughput: 293.333 infer/sec. Avg latency: 137567 usec (std 356 usec)\n",
      "  Pass [3] throughput: 293.333 infer/sec. Avg latency: 137471 usec (std 407 usec)\n",
      "  Client: \n",
      "    Request count: 110\n",
      "    Throughput: 293.333 infer/sec\n",
      "    Avg latency: 137471 usec (standard deviation 407 usec)\n",
      "    p50 latency: 137472 usec\n",
      "    p90 latency: 137998 usec\n",
      "    p95 latency: 138206 usec\n",
      "    p99 latency: 138361 usec\n",
      "    Avg HTTP time: 137444 usec (send 11 usec + response wait 137432 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1048\n",
      "    Execution count: 131\n",
      "    Successful request count: 131\n",
      "    Avg request latency: 137025 usec (overhead 2 usec + queue 109621 usec + compute input 17 usec + compute infer 27372 usec + compute output 13 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 288 infer/sec. Avg latency: 165241 usec (std 387 usec)\n",
      "  Pass [2] throughput: 290.667 infer/sec. Avg latency: 165309 usec (std 457 usec)\n",
      "  Pass [3] throughput: 288 infer/sec. Avg latency: 165701 usec (std 351 usec)\n",
      "  Client: \n",
      "    Request count: 108\n",
      "    Throughput: 288 infer/sec\n",
      "    Avg latency: 165701 usec (standard deviation 351 usec)\n",
      "    p50 latency: 165627 usec\n",
      "    p90 latency: 166176 usec\n",
      "    p95 latency: 166356 usec\n",
      "    p99 latency: 166716 usec\n",
      "    Avg HTTP time: 165628 usec (send 12 usec + response wait 165615 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1040\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 165211 usec (overhead 3 usec + queue 137689 usec + compute input 17 usec + compute infer 27489 usec + compute output 13 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 290.667 infer/sec. Avg latency: 193328 usec (std 417 usec)\n",
      "  Pass [2] throughput: 290.667 infer/sec. Avg latency: 193457 usec (std 565 usec)\n",
      "  Pass [3] throughput: 290.667 infer/sec. Avg latency: 193535 usec (std 578 usec)\n",
      "  Client: \n",
      "    Request count: 109\n",
      "    Throughput: 290.667 infer/sec\n",
      "    Avg latency: 193535 usec (standard deviation 578 usec)\n",
      "    p50 latency: 193423 usec\n",
      "    p90 latency: 194282 usec\n",
      "    p95 latency: 194682 usec\n",
      "    p99 latency: 195085 usec\n",
      "    Avg HTTP time: 193542 usec (send 19 usec + response wait 193522 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1040\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 193086 usec (overhead 3 usec + queue 165520 usec + compute input 20 usec + compute infer 27529 usec + compute output 14 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 290.667 infer/sec. Avg latency: 221135 usec (std 1576 usec)\n",
      "  Pass [2] throughput: 288 infer/sec. Avg latency: 221714 usec (std 1268 usec)\n",
      "  Pass [3] throughput: 288 infer/sec. Avg latency: 221804 usec (std 1365 usec)\n",
      "  Client: \n",
      "    Request count: 108\n",
      "    Throughput: 288 infer/sec\n",
      "    Avg latency: 221804 usec (standard deviation 1365 usec)\n",
      "    p50 latency: 221466 usec\n",
      "    p90 latency: 223258 usec\n",
      "    p95 latency: 225854 usec\n",
      "    p99 latency: 226198 usec\n",
      "    Avg HTTP time: 221646 usec (send 17 usec + response wait 221628 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1040\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 221204 usec (overhead 2 usec + queue 193582 usec + compute input 20 usec + compute infer 27587 usec + compute output 13 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 288 infer/sec. Avg latency: 248986 usec (std 3316 usec)\n",
      "  Pass [2] throughput: 290.667 infer/sec. Avg latency: 249289 usec (std 553 usec)\n",
      "  Pass [3] throughput: 290.667 infer/sec. Avg latency: 249620 usec (std 881 usec)\n",
      "  Client: \n",
      "    Request count: 109\n",
      "    Throughput: 290.667 infer/sec\n",
      "    Avg latency: 249620 usec (standard deviation 881 usec)\n",
      "    p50 latency: 249522 usec\n",
      "    p90 latency: 250888 usec\n",
      "    p95 latency: 251508 usec\n",
      "    p99 latency: 251800 usec\n",
      "    Avg HTTP time: 249691 usec (send 17 usec + response wait 249673 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1040\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 249255 usec (overhead 3 usec + queue 221588 usec + compute input 19 usec + compute infer 27632 usec + compute output 13 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 290.667 infer/sec. Avg latency: 276141 usec (std 4595 usec)\n",
      "  Pass [2] throughput: 288 infer/sec. Avg latency: 277221 usec (std 706 usec)\n",
      "  Pass [3] throughput: 290.667 infer/sec. Avg latency: 277389 usec (std 682 usec)\n",
      "  Client: \n",
      "    Request count: 109\n",
      "    Throughput: 290.667 infer/sec\n",
      "    Avg latency: 277389 usec (standard deviation 682 usec)\n",
      "    p50 latency: 277268 usec\n",
      "    p90 latency: 278321 usec\n",
      "    p95 latency: 278683 usec\n",
      "    p99 latency: 278774 usec\n",
      "    Avg HTTP time: 277547 usec (send 17 usec + response wait 277529 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1032\n",
      "    Execution count: 129\n",
      "    Successful request count: 129\n",
      "    Avg request latency: 277107 usec (overhead 3 usec + queue 249442 usec + compute input 19 usec + compute infer 27630 usec + compute output 13 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 290.667 infer/sec, latency 27667 usec\n",
      "Concurrency: 2, throughput: 290.667 infer/sec, latency 54912 usec\n",
      "Concurrency: 3, throughput: 293.333 infer/sec, latency 82439 usec\n",
      "Concurrency: 4, throughput: 290.667 infer/sec, latency 109858 usec\n",
      "Concurrency: 5, throughput: 293.333 infer/sec, latency 137471 usec\n",
      "Concurrency: 6, throughput: 288 infer/sec, latency 165701 usec\n",
      "Concurrency: 7, throughput: 290.667 infer/sec, latency 193535 usec\n",
      "Concurrency: 8, throughput: 288 infer/sec, latency 221804 usec\n",
      "Concurrency: 9, throughput: 290.667 infer/sec, latency 249620 usec\n",
      "Concurrency: 10, throughput: 290.667 infer/sec, latency 277389 usec\n"
     ]
    }
   ],
   "source": [
    "modelName = \"bertQA-onnx-trt-fp16\"\n",
    "maxConcurrency= \"10\"\n",
    "batchSize=\"8\"\n",
    "print(\"Running: \" + modelName)\n",
    "!bash ./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compare the performance against ONNX runtime. \n",
    "* How did the latency change, especially across larger concurrency runs? \n",
    "* How did the bandwidth change? Can you explain the level of bandwidth change observed? \n",
    "* Why did the ONNX model timeout at concurrency of less than 10? How does the TensorRT latency at concurrency 10 compare to latency of pure ONNX runtime at an earlier concurrency?\n",
    "\n",
    "Discuss with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Congratulations!</h3><br>\n",
    "You've successfully deployed an NLP model to Triton Server with TorchScript and applied both reduced precision and TensorRT optimizations.\n",
    "In the next notebook you'll learn how to optimize the model itself and to deploy it in an efficient way. \n",
    "\n",
    "Please proceed to the next notebook:<br>\n",
    "[2.0 Hosting the model](020_HostingTheModel.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
