{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Hosting the Model\n",
    "\n",
    "In this notebook, you'll learn strategies to optimize Triton Server to improve the performance of your deployment.\n",
    "\n",
    "\n",
    "**[2.1 Concurrent Model Execution](#2.1-Concurrent-Model-Execution)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.1 Exercise: Usage Considerations](#2.1.1-Exercise:-Usage-Considerations)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.2 Implementation](#2.1.2-Implementation)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.3 Exercise: Configure Multiple Instance Groups](#2.1.3-Exercise:-Configure-Multiple-Instance-Groups)<br>\n",
    "**[2.2 Scheduling Strategies](#2.2-Scheduling-Strategies)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.1 Stateless Inference](#2.2.1-Stateless-Inference)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.2 Stateful Inference](#2.2.2-Stateful-Inference)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.3 Pipelines / Ensembles](#2.2.3-Pipelines-/-Ensembles)<br>\n",
    "**[2.3 Dynamic Batching](#2.3-Dynamic-Batching)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.3.1 Exercise: Implement Dynamic Batching](#2.3.1-Exercise:-Implement-Dynamic-Batching)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've executed customer requests sequentially, in the order they have arrived at the server, and used a static batch of size 8 for any requests to our server. This has not only left our GPUs heavily underutilized, but has also significantly affected the latency of responses received from the server. This is not an uncommon situation. Unless you are developing an application that processes large volumes of data in batch, you will likely be sending individual inference requests from the user application, leading to even further underutilization. As we have seen in the previous notebook, model optimizations do help considerably to accelerate model execution.  However, they do not change the fact that when serving is implemented naively, the nature of the inference workload leads to GPU underutilization.\n",
    "\n",
    "Inference servers, such as NVIDIA Triton, implement a wide range of features that allow us to improve the GPU utilization and improve request latency. The three that we will discuss in this class are:<br/>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/architecture.html#section-concurrent-model-execution\">Concurrent model execution</a></br>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/models_and_schedulers.html\">Scheduling</a> <br/>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/model_configuration.html#section-dynamic-batcher\">Dynamic batching</a> <br/>\n",
    "\n",
    "\n",
    "Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/quickstart.html\">Triton documentation</a> and its <a href=\"https://github.com/NVIDIA/triton-inference-server\">source code</a> for further information about the mechanisms and configurations that can help improve model inference performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Concurrent Model Execution\n",
    "The Triton architecture allows multiple models and/or multiple instances of the same model to execute in parallel on a single GPU. The following figure shows an example with two models: `model0` and `model1`. Assuming Triton is not currently processing any request, when two requests arrive simultaneously, one for each model, Triton immediately schedules both of them onto the GPU, and the GPUâ€™s hardware scheduler begins working on both computations in parallel. </br>\n",
    "\n",
    "<img src=\"images/multi_model_exec.png\"/><br/>\n",
    "\n",
    "#### Default Behavior\n",
    "\n",
    "By default, if multiple requests for the same model arrive at the same time, Triton will serialize their execution by scheduling only one at a time on the GPU, as shown in the following figure.\n",
    "\n",
    "<img src=\"images/multi_model_serial_exec.png\"/><br/>\n",
    "\n",
    "Triton provides an instance-group feature that allows each model to specify how many parallel executions of that model should be allowed. Each such enabled parallel execution is referred to as an *execution instance*. By default, Triton gives each model a single execution instance, which means that only a single execution of the model is allowed to be in progress at a time as shown in the above figure. \n",
    "\n",
    "#### Instance Groups\n",
    "By using the *instance-group* setting, the number of execution instances for a model can be increased. The following figure shows model execution when `model1` is configured to allow three execution instances. As shown in the figure, the first three `model1` inference requests are immediately executed in parallel on the GPU. The fourth `model1` inference request must wait until one of the first three executions completes before beginning.\n",
    "\n",
    "<img src=\"images/multi_model_parallel_exec.png\"/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Exercise: Usage Considerations\n",
    "\n",
    "For most models, the Triton feature that provides the largest performance improvement is *dynamic batching*. The key advantages of dynamic batching over setting up multiple instance execution are:\n",
    "- No overhead for model parameter storage\n",
    "- No overhead related to model parameter fetch from the GPU memory\n",
    "- Better utilization of the GPU resources\n",
    "\n",
    "Before we look at the configuration for multiple model execution, let's execute our model again using a single instance, and observe the resource utilization of the GPU. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Steps\n",
    "1. Launch a terminal window from the JupyterLab launch page.  If you need to open a new launch page, click the '+' icon on the left sidebar menu. You can then use a drag-and-drop action to move the terminal to a sub-window configuration  for better viewing.\n",
    "2. Execute the following command in the terminal before you run the performance tool:<br>\n",
    "\n",
    "```\n",
    "watch -n0.5 nvidia-smi\n",
    "```\n",
    "    You should see an output that resembles:\n",
    "<img src=\"images/NVIDIASMI.png\" style=\"position:relative; left:30px;\" width=800/>\n",
    "\n",
    "3. Execute the same benchmark we used in the previous notebook, but with the batch size reduced to 1, and observe the <code>nvidia-smi</code> output again.  Pay special attention to the memory consumption and GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n"
     ]
    }
   ],
   "source": [
    "# Set the server hostname and check it - you should get a message that \"Triton Server is ready!\"\n",
    "tritonServerHostName = \"triton\"\n",
    "!./utilities/wait_for_triton_server.sh {tritonServerHostName}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previous configuration.\n",
    "modelVersion=\"1\"\n",
    "precision=\"fp32\"\n",
    "batchSize=\"1\"\n",
    "maxLatency=\"500\"\n",
    "maxClientThreads=\"10\"\n",
    "maxConcurrency=\"2\"\n",
    "dockerBridge=\"host\"\n",
    "resultsFolderName=\"1\"\n",
    "profilingData=\"utilities/profiling_data_int64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-fp16\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "error: failed to get model metadata: Request for unknown model: 'bertQA-onnx-trt-fp16' is not found\n"
     ]
    }
   ],
   "source": [
    "# Update configuration parameters and run profiler.\n",
    "modelName = \"bertQA-onnx-trt-fp16\"\n",
    "maxConcurrency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \" + modelName)\n",
    "!bash ./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you have observed utilization similar to the following:<br/>\n",
    "<img src=\"images/NVIDIASMI2.png\" width=800/><br/>\n",
    "\n",
    "Do you think you will observe a major acceleration as a consequence of increasing the number of instance groups?<br>\n",
    "Discuss with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 Implementation\n",
    "Let's look at how to enable concurrent execution and what impact it will have on our model performance. Execute the following code cells to export the model in the ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-conexec\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-conexec in format onnxruntime_onnx\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1336539973\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.00022935867309570312\n",
      "\n",
      "average L_inf error over output tensors:  0.0001423954963684082\n",
      "variance of L_inf error over output tensors:  6.657553323445124e-09\n",
      "stddev of L_inf error over output tensors:  8.159383140559784e-05\n",
      "\n",
      "time of error check of native model:  0.4294290542602539 seconds\n",
      "time of error check of onnx model:  19.680660247802734 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16K\n",
      "drwxr-xr-x 3 root root 4.0K Sep 14 15:44 .\n",
      "drwxr-xr-x 3 root root 4.0K Sep 14 15:43 ..\n",
      "drwxr-xr-x 2 root root 4.0K Sep 14 15:43 1\n",
      "-rw-r--r-- 1 root root  569 Sep 14 15:44 config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!ls -alh ./candidatemodels/bertQA-onnx-conexec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3 Exercise: Configure Multiple Instance Groups\n",
    "In order to specify multiple instances, we need to change the \"count\" value from '1' to a larger number in the `instance_group` section of the \"config.pbtxt\" configuration file. \n",
    "\n",
    "\n",
    "```\n",
    "    instance_group [\n",
    "    {\n",
    "        count: 2\n",
    "        kind: KIND_GPU\n",
    "        gpus: [ 0 ]\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "#### Exercise Steps:\n",
    "1. Modify [config.pbtxt](candidatemodels/bertQA-onnx-conexec/config.pbtxt) in the `bertQA-onnx-conexec` deployment just created to specify two instances of our BERT-based question answering model. You should find the default instance_group block at the end of the file. Change the count variable from 1 to 2.  (see the [solution](solutions/ex-2-1-3_config.pbtxt) as needed)\n",
    "2. To make the comparison fair, also enable TensorRT with the addition of an `execution_accelerators` block inside the `optimization` block:\n",
    "\n",
    "```text\n",
    "optimization {\n",
    "   execution_accelerators {\n",
    "      gpu_execution_accelerator : [ {\n",
    "         name : \"tensorrt\"\n",
    "         parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "      }]\n",
    "   }\n",
    "cuda { graphs: 0 }\n",
    "}\n",
    "```\n",
    "\n",
    "3. Once you have saved your changes (Main menu: File -> Save File), move the model across to Triton by executing the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx-conexec model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run our standard stress test against the model. Please compare it to the single instance execution.<br>\n",
    "   Did the throughput change?<br>\n",
    "   Did the latency change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-conexec\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      ".................................................................................................Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 36.3333 infer/sec. Avg latency: 27639 usec (std 257 usec)\n",
      "  Pass [2] throughput: 36 infer/sec. Avg latency: 27625 usec (std 122 usec)\n",
      "  Pass [3] throughput: 36.3333 infer/sec. Avg latency: 27639 usec (std 123 usec)\n",
      "  Client: \n",
      "    Request count: 109\n",
      "    Throughput: 36.3333 infer/sec\n",
      "    Avg latency: 27639 usec (standard deviation 123 usec)\n",
      "    p50 latency: 27599 usec\n",
      "    p90 latency: 27797 usec\n",
      "    p95 latency: 28010 usec\n",
      "    p99 latency: 28068 usec\n",
      "    Avg HTTP time: 27642 usec (send 5 usec + response wait 27636 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 130\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 27312 usec (overhead 3 usec + queue 30 usec + compute input 12 usec + compute infer 27254 usec + compute output 13 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 36.6667 infer/sec. Avg latency: 54725 usec (std 2742 usec)\n",
      "  Pass [2] throughput: 36.3333 infer/sec. Avg latency: 54894 usec (std 5313 usec)\n",
      "  Pass [3] throughput: 36.6667 infer/sec. Avg latency: 54784 usec (std 2643 usec)\n",
      "  Client: \n",
      "    Request count: 110\n",
      "    Throughput: 36.6667 infer/sec\n",
      "    Avg latency: 54784 usec (standard deviation 2643 usec)\n",
      "    p50 latency: 54863 usec\n",
      "    p90 latency: 57864 usec\n",
      "    p95 latency: 59244 usec\n",
      "    p99 latency: 61231 usec\n",
      "    Avg HTTP time: 54789 usec (send 7 usec + response wait 54781 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 132\n",
      "    Execution count: 132\n",
      "    Successful request count: 132\n",
      "    Avg request latency: 54406 usec (overhead 2 usec + queue 34 usec + compute input 14 usec + compute infer 54340 usec + compute output 16 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 36.3333 infer/sec. Avg latency: 82084 usec (std 13994 usec)\n",
      "  Pass [2] throughput: 36.6667 infer/sec. Avg latency: 82239 usec (std 13686 usec)\n",
      "  Pass [3] throughput: 36.3333 infer/sec. Avg latency: 82317 usec (std 12880 usec)\n",
      "  Client: \n",
      "    Request count: 109\n",
      "    Throughput: 36.3333 infer/sec\n",
      "    Avg latency: 82317 usec (standard deviation 12880 usec)\n",
      "    p50 latency: 81327 usec\n",
      "    p90 latency: 98602 usec\n",
      "    p95 latency: 103034 usec\n",
      "    p99 latency: 105248 usec\n",
      "    Avg HTTP time: 82333 usec (send 7 usec + response wait 82324 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 132\n",
      "    Execution count: 132\n",
      "    Successful request count: 132\n",
      "    Avg request latency: 81947 usec (overhead 4 usec + queue 27186 usec + compute input 11 usec + compute infer 54732 usec + compute output 14 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 36.6667 infer/sec. Avg latency: 109625 usec (std 5948 usec)\n",
      "  Pass [2] throughput: 36.3333 infer/sec. Avg latency: 109747 usec (std 4574 usec)\n",
      "  Pass [3] throughput: 36.3333 infer/sec. Avg latency: 109987 usec (std 4116 usec)\n",
      "  Client: \n",
      "    Request count: 109\n",
      "    Throughput: 36.3333 infer/sec\n",
      "    Avg latency: 109987 usec (standard deviation 4116 usec)\n",
      "    p50 latency: 109956 usec\n",
      "    p90 latency: 114845 usec\n",
      "    p95 latency: 117179 usec\n",
      "    p99 latency: 120354 usec\n",
      "    Avg HTTP time: 109919 usec (send 8 usec + response wait 109910 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 131\n",
      "    Execution count: 131\n",
      "    Successful request count: 131\n",
      "    Avg request latency: 109517 usec (overhead 3 usec + queue 54662 usec + compute input 13 usec + compute infer 54824 usec + compute output 15 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 36.3333 infer/sec. Avg latency: 137545 usec (std 14745 usec)\n",
      "  Pass [2] throughput: 36.3333 infer/sec. Avg latency: 137763 usec (std 14362 usec)\n",
      "  Pass [3] throughput: 36.3333 infer/sec. Avg latency: 137819 usec (std 17700 usec)\n",
      "  Client: \n",
      "    Request count: 109\n",
      "    Throughput: 36.3333 infer/sec\n",
      "    Avg latency: 137819 usec (standard deviation 17700 usec)\n",
      "    p50 latency: 138342 usec\n",
      "    p90 latency: 161136 usec\n",
      "    p95 latency: 163530 usec\n",
      "    p99 latency: 165140 usec\n",
      "    Avg HTTP time: 137737 usec (send 7 usec + response wait 137729 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 131\n",
      "    Execution count: 131\n",
      "    Successful request count: 131\n",
      "    Avg request latency: 137334 usec (overhead 3 usec + queue 82344 usec + compute input 17 usec + compute infer 54952 usec + compute output 18 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 36.3333 infer/sec. Avg latency: 165082 usec (std 6892 usec)\n",
      "  Pass [2] throughput: 36.3333 infer/sec. Avg latency: 164795 usec (std 5849 usec)\n",
      "  Pass [3] throughput: 34 infer/sec. Avg latency: 176372 usec (std 8810 usec)\n",
      "  Client: \n",
      "    Request count: 102\n",
      "    Throughput: 34 infer/sec\n",
      "    Avg latency: 176372 usec (standard deviation 8810 usec)\n",
      "    p50 latency: 178380 usec\n",
      "    p90 latency: 186861 usec\n",
      "    p95 latency: 189084 usec\n",
      "    p99 latency: 191178 usec\n",
      "    Avg HTTP time: 174251 usec (send 10 usec + response wait 174239 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 124\n",
      "    Execution count: 124\n",
      "    Successful request count: 124\n",
      "    Avg request latency: 173501 usec (overhead 4 usec + queue 115617 usec + compute input 19 usec + compute infer 57836 usec + compute output 25 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 36.3333 infer/sec. Avg latency: 192266 usec (std 13688 usec)\n",
      "  Pass [2] throughput: 36.3333 infer/sec. Avg latency: 192131 usec (std 5917 usec)\n",
      "  Pass [3] throughput: 36.3333 infer/sec. Avg latency: 192368 usec (std 11349 usec)\n",
      "  Client: \n",
      "    Request count: 109\n",
      "    Throughput: 36.3333 infer/sec\n",
      "    Avg latency: 192368 usec (standard deviation 11349 usec)\n",
      "    p50 latency: 191935 usec\n",
      "    p90 latency: 205311 usec\n",
      "    p95 latency: 210039 usec\n",
      "    p99 latency: 220401 usec\n",
      "    Avg HTTP time: 192193 usec (send 8 usec + response wait 192183 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 131\n",
      "    Execution count: 131\n",
      "    Successful request count: 131\n",
      "    Avg request latency: 191786 usec (overhead 3 usec + queue 136965 usec + compute input 13 usec + compute infer 54790 usec + compute output 15 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 36.6667 infer/sec. Avg latency: 219934 usec (std 9347 usec)\n",
      "  Pass [2] throughput: 36.3333 infer/sec. Avg latency: 220239 usec (std 11293 usec)\n",
      "  Pass [3] throughput: 36 infer/sec. Avg latency: 220413 usec (std 11666 usec)\n",
      "  Client: \n",
      "    Request count: 108\n",
      "    Throughput: 36 infer/sec\n",
      "    Avg latency: 220413 usec (standard deviation 11666 usec)\n",
      "    p50 latency: 220247 usec\n",
      "    p90 latency: 234155 usec\n",
      "    p95 latency: 238857 usec\n",
      "    p99 latency: 243969 usec\n",
      "    Avg HTTP time: 220191 usec (send 8 usec + response wait 220181 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 130\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 219756 usec (overhead 4 usec + queue 164818 usec + compute input 17 usec + compute infer 54899 usec + compute output 18 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 36.3333 infer/sec. Avg latency: 246666 usec (std 12749 usec)\n",
      "  Pass [2] throughput: 36.3333 infer/sec. Avg latency: 247579 usec (std 9935 usec)\n",
      "  Pass [3] throughput: 36.3333 infer/sec. Avg latency: 247595 usec (std 9741 usec)\n",
      "  Client: \n",
      "    Request count: 109\n",
      "    Throughput: 36.3333 infer/sec\n",
      "    Avg latency: 247595 usec (standard deviation 9741 usec)\n",
      "    p50 latency: 247890 usec\n",
      "    p90 latency: 261126 usec\n",
      "    p95 latency: 264041 usec\n",
      "    p99 latency: 269686 usec\n",
      "    Avg HTTP time: 247578 usec (send 8 usec + response wait 247569 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 131\n",
      "    Execution count: 131\n",
      "    Successful request count: 131\n",
      "    Avg request latency: 247166 usec (overhead 3 usec + queue 192234 usec + compute input 15 usec + compute infer 54900 usec + compute output 14 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 36.3333 infer/sec. Avg latency: 274382 usec (std 7265 usec)\n",
      "  Pass [2] throughput: 36.3333 infer/sec. Avg latency: 274802 usec (std 8628 usec)\n",
      "  Pass [3] throughput: 36.3333 infer/sec. Avg latency: 275309 usec (std 4850 usec)\n",
      "  Client: \n",
      "    Request count: 109\n",
      "    Throughput: 36.3333 infer/sec\n",
      "    Avg latency: 275309 usec (standard deviation 4850 usec)\n",
      "    p50 latency: 275099 usec\n",
      "    p90 latency: 280596 usec\n",
      "    p95 latency: 282622 usec\n",
      "    p99 latency: 289077 usec\n",
      "    Avg HTTP time: 275073 usec (send 9 usec + response wait 275062 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 131\n",
      "    Execution count: 131\n",
      "    Successful request count: 131\n",
      "    Avg request latency: 274647 usec (overhead 3 usec + queue 219711 usec + compute input 14 usec + compute infer 54904 usec + compute output 15 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 36.3333 infer/sec, latency 27639 usec\n",
      "Concurrency: 2, throughput: 36.6667 infer/sec, latency 54784 usec\n",
      "Concurrency: 3, throughput: 36.3333 infer/sec, latency 82317 usec\n",
      "Concurrency: 4, throughput: 36.3333 infer/sec, latency 109987 usec\n",
      "Concurrency: 5, throughput: 36.3333 infer/sec, latency 137819 usec\n",
      "Concurrency: 6, throughput: 34 infer/sec, latency 176372 usec\n",
      "Concurrency: 7, throughput: 36.3333 infer/sec, latency 192368 usec\n",
      "Concurrency: 8, throughput: 36 infer/sec, latency 220413 usec\n",
      "Concurrency: 9, throughput: 36.3333 infer/sec, latency 247595 usec\n",
      "Concurrency: 10, throughput: 36.3333 infer/sec, latency 275309 usec\n"
     ]
    }
   ],
   "source": [
    "maxConcurrency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \" + modelName)\n",
    "!bash ./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, let's free up some GPU memory by moving some of the models out of the Triton model repository.  After removing the following three models, only the `bertQA-torchscript` model should remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '/dli/task/model_repository/bertQA-onnx': No such file or directory\n",
      "mv: cannot stat '/dli/task/model_repository/bertQA-onnx-trt-fp16': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Remove models from the inference server by removing them from the model_repository\n",
    "!mv /dli/task/model_repository/bertQA-onnx /dli/task/candidatemodels/\n",
    "!mv /dli/task/model_repository/bertQA-onnx-conexec /dli/task/candidatemodels/\n",
    "!mv /dli/task/model_repository/bertQA-onnx-trt-fp16 /dli/task/candidatemodels/\n",
    "\n",
    "# List remaining models on the inference server\n",
    "!ls /dli/task/model_repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Scheduling Strategies\n",
    "Triton supports batch inferencing by allowing individual inference requests to specify a batch of inputs. The inferencing for a batch of inputs is performed at the same time which is especially important for GPUs since it can greatly increase inferencing throughput. In many use cases the individual inference requests are not batched, therefore, they do not benefit from the throughput benefits of batching. <br/>\n",
    "\n",
    "The inference server contains multiple scheduling and batching algorithms that support many different model types and use-cases. The choice of the scheduler / batcher will be driven by several factors the key ones being:\n",
    "- Stateful / stateless nature of your inference workload\n",
    "- Whether your application is composed of models served in isolation or whether a more complex pipeline / ensemble is being used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 Stateless Inference\n",
    "\n",
    "When dealing with stateless inference (as we are in this class) we have two main options when it comes to scheduling. The first option is the default scheduler which will distribute request to all instances assigned for inference. This is the preferred option when the structure of the inference workload is well understood and where inference will take place at regular batch sizes and time intervals.\n",
    "\n",
    "The second option is dynamic batching which combines individual request and similarly to the default batcher distributes the larges batches across instances. We will discuss this particular option in the next section of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Stateful Inference\n",
    "\n",
    "A stateful model (or stateful custom backend) does maintain state between inference requests. The model is expecting multiple inference requests that together form a sequence of inferences that must be routed to the same model instance so that the state being maintained by the model is correctly updated. Moreover, the model may require that Triton provide control signals indicating, for example, sequence start.\n",
    "\n",
    "The sequence batcher can employ one of two scheduling strategies when deciding how to batch the sequences that are routed to the same model instance. These strategies are Direct and Oldest.\n",
    "\n",
    "With the Direct scheduling strategy the sequence batcher ensures not only that all inference requests in a sequence are routed to the same model instance, but also that each sequence is routed to a dedicated batch slot within the model instance. This strategy is required when the model maintains state for each batch slot, and is expecting all inference requests for a given sequence to be routed to the same slot so that the state is correctly updated.\n",
    "\n",
    "With the Oldest scheduling strategy the sequence batcher ensures that all inference requests in a sequence are routed to the same model instance and then uses the dynamic batcher to batch together multiple inferences from different sequences into a batch that inferences together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3 Pipelines / Ensembles\n",
    "\n",
    "An ensemble model represents a pipeline of one or more models and the connection of input and output tensors between those models. Ensemble models are intended to be used to encapsulate a procedure that involves multiple models, such as \"data preprocessing -> inference -> data post-processing\". Using ensemble models for this purpose can avoid the overhead of transferring intermediate tensors and minimize the number of requests that must be sent to Triton. An example of an ensemble pipeline is illustrated below: <br/>\n",
    "\n",
    "<img src=\"images/ensemble_example0.png\"/>\n",
    "\n",
    "The ensemble scheduler must be used for ensemble models, regardless of the scheduler used by the models within the ensemble. With respect to the ensemble scheduler, an ensemble model is not an actual model. Instead, it specifies the data flow between models within the ensemble as Step. The scheduler collects the output tensors in each step, provides them as input tensors for other steps according to the specification. In spite of that, the ensemble model is still viewed as a single model from an external view.\n",
    "\n",
    "More information on Triton scheduling can be found in the <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/models_and_schedulers.html#stateless-models\">following section of the documentation</a>. In this class, we will focus further on one of the most powerful features of Triton, *dynamic batching*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Dynamic Batching\n",
    "Dynamic batching is a feature of Triton that allows inference requests to be combined by the server, so that a batch is created dynamically, resulting in increased throughput.\n",
    "\n",
    "When a model instance becomes available for inferencing, the dynamic batcher will attempt to create batches from the requests that are available in the scheduler. Requests are added to the batch in the order the requests were received. If the dynamic batcher can form a batch of a preferred size(s) it will create a batch of the largest possible preferred size and send it for inferencing. If the dynamic batcher cannot form a batch of a preferred size, it will send a batch of the largest size possible that is less than the max batch size allowed by the model. \n",
    "\n",
    "The dynamic batcher can be configured to allow requests to be delayed for a limited time in the scheduler to allow other requests to join the dynamic batch. For example, the following configuration sets the maximum delay time of 100 microseconds for a request:\n",
    "\n",
    "```\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [ 4, 8 ]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 Exercise: Implement Dynamic Batching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin again by exporting an ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-trt-dynbatch\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-trt-dynbatch in format onnxruntime_onnx\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1336539973\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.00022935867309570312\n",
      "\n",
      "average L_inf error over output tensors:  0.0001423954963684082\n",
      "variance of L_inf error over output tensors:  6.657553323445124e-09\n",
      "stddev of L_inf error over output tensors:  8.159383140559784e-05\n",
      "\n",
      "time of error check of native model:  0.40944528579711914 seconds\n",
      "time of error check of onnx model:  16.858489274978638 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Steps\n",
    "1. Modify [config.pbtxt](candidatemodels/bertQA-onnx-trt-dynbatch/config.pbtxt) for dynamic batching using the example snippet. \n",
    "\n",
    "    ```\n",
    "    dynamic_batching {\n",
    "      preferred_batch_size: [ 4, 8 ]\n",
    "      max_queue_delay_microseconds: 100\n",
    "    }\n",
    "    ```\n",
    "    \n",
    "2. Enable TensorRT in the optimization block.\n",
    "\n",
    "    ```\n",
    "    optimization {\n",
    "       execution_accelerators {\n",
    "          gpu_execution_accelerator : [ {\n",
    "             name : \"tensorrt\"\n",
    "             parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "          }]\n",
    "       }\n",
    "    cuda { graphs: 0 }\n",
    "    }\n",
    "    ```\n",
    "3. Once saved, move the model to the Triton model repository and run the performance utility by executing the following cells. ([solution](solutions/ex-2-3-1_config.pbtxt) if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx-trt-dynbatch model_repository/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-dynbatch\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "...................Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 36 infer/sec. Avg latency: 27775 usec (std 159 usec)\n",
      "  Pass [2] throughput: 35.6667 infer/sec. Avg latency: 27789 usec (std 136 usec)\n",
      "  Pass [3] throughput: 36 infer/sec. Avg latency: 27763 usec (std 118 usec)\n",
      "  Client: \n",
      "    Request count: 108\n",
      "    Throughput: 36 infer/sec\n",
      "    Avg latency: 27763 usec (standard deviation 118 usec)\n",
      "    p50 latency: 27729 usec\n",
      "    p90 latency: 27879 usec\n",
      "    p95 latency: 28146 usec\n",
      "    p99 latency: 28239 usec\n",
      "    Avg HTTP time: 27752 usec (send 5 usec + response wait 27746 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 129\n",
      "    Execution count: 129\n",
      "    Successful request count: 129\n",
      "    Avg request latency: 27412 usec (overhead 2 usec + queue 184 usec + compute input 12 usec + compute infer 27200 usec + compute output 14 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 36.3333 infer/sec. Avg latency: 54814 usec (std 287 usec)\n",
      "  Pass [2] throughput: 36.3333 infer/sec. Avg latency: 54793 usec (std 199 usec)\n",
      "  Pass [3] throughput: 36.6667 infer/sec. Avg latency: 54770 usec (std 193 usec)\n",
      "  Client: \n",
      "    Request count: 110\n",
      "    Throughput: 36.6667 infer/sec\n",
      "    Avg latency: 54770 usec (standard deviation 193 usec)\n",
      "    p50 latency: 54709 usec\n",
      "    p90 latency: 54933 usec\n",
      "    p95 latency: 55308 usec\n",
      "    p99 latency: 55487 usec\n",
      "    Avg HTTP time: 54751 usec (send 6 usec + response wait 54744 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 132\n",
      "    Execution count: 132\n",
      "    Successful request count: 132\n",
      "    Avg request latency: 54391 usec (overhead 3 usec + queue 27106 usec + compute input 9 usec + compute infer 27256 usec + compute output 17 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 54.3333 infer/sec. Avg latency: 54966 usec (std 237 usec)\n",
      "  Pass [2] throughput: 54.6667 infer/sec. Avg latency: 54976 usec (std 279 usec)\n",
      "  Pass [3] throughput: 54.3333 infer/sec. Avg latency: 55077 usec (std 287 usec)\n",
      "  Client: \n",
      "    Request count: 163\n",
      "    Throughput: 54.3333 infer/sec\n",
      "    Avg latency: 55077 usec (standard deviation 287 usec)\n",
      "    p50 latency: 55084 usec\n",
      "    p90 latency: 55455 usec\n",
      "    p95 latency: 55632 usec\n",
      "    p99 latency: 55768 usec\n",
      "    Avg HTTP time: 55096 usec (send 7 usec + response wait 55088 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 195\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 54675 usec (overhead 4 usec + queue 27242 usec + compute input 13 usec + compute infer 27394 usec + compute output 22 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 72.3333 infer/sec. Avg latency: 55023 usec (std 339 usec)\n",
      "  Pass [2] throughput: 73 infer/sec. Avg latency: 55081 usec (std 305 usec)\n",
      "  Pass [3] throughput: 72.3333 infer/sec. Avg latency: 55069 usec (std 313 usec)\n",
      "  Client: \n",
      "    Request count: 217\n",
      "    Throughput: 72.3333 infer/sec\n",
      "    Avg latency: 55069 usec (standard deviation 313 usec)\n",
      "    p50 latency: 54935 usec\n",
      "    p90 latency: 55513 usec\n",
      "    p95 latency: 55573 usec\n",
      "    p99 latency: 56058 usec\n",
      "    Avg HTTP time: 55039 usec (send 7 usec + response wait 55031 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 261\n",
      "    Execution count: 131\n",
      "    Successful request count: 131\n",
      "    Avg request latency: 54577 usec (overhead 3 usec + queue 27186 usec + compute input 15 usec + compute infer 27347 usec + compute output 26 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 90 infer/sec. Avg latency: 55227 usec (std 311 usec)\n",
      "  Pass [2] throughput: 91 infer/sec. Avg latency: 55020 usec (std 213 usec)\n",
      "  Pass [3] throughput: 90 infer/sec. Avg latency: 55154 usec (std 279 usec)\n",
      "  Client: \n",
      "    Request count: 270\n",
      "    Throughput: 90 infer/sec\n",
      "    Avg latency: 55154 usec (standard deviation 279 usec)\n",
      "    p50 latency: 55067 usec\n",
      "    p90 latency: 55577 usec\n",
      "    p95 latency: 55625 usec\n",
      "    p99 latency: 55970 usec\n",
      "    Avg HTTP time: 55150 usec (send 7 usec + response wait 55142 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 325\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 54676 usec (overhead 3 usec + queue 27250 usec + compute input 14 usec + compute infer 27390 usec + compute output 19 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 108.667 infer/sec. Avg latency: 55136 usec (std 284 usec)\n",
      "  Pass [2] throughput: 108 infer/sec. Avg latency: 55346 usec (std 334 usec)\n",
      "  Pass [3] throughput: 109.333 infer/sec. Avg latency: 55244 usec (std 512 usec)\n",
      "  Client: \n",
      "    Request count: 328\n",
      "    Throughput: 109.333 infer/sec\n",
      "    Avg latency: 55244 usec (standard deviation 512 usec)\n",
      "    p50 latency: 55202 usec\n",
      "    p90 latency: 55673 usec\n",
      "    p95 latency: 55760 usec\n",
      "    p99 latency: 56019 usec\n",
      "    Avg HTTP time: 55244 usec (send 7 usec + response wait 55235 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 394\n",
      "    Execution count: 131\n",
      "    Successful request count: 131\n",
      "    Avg request latency: 54684 usec (overhead 3 usec + queue 27225 usec + compute input 16 usec + compute infer 27416 usec + compute output 24 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 126 infer/sec. Avg latency: 55241 usec (std 353 usec)\n",
      "  Pass [2] throughput: 126 infer/sec. Avg latency: 55261 usec (std 322 usec)\n",
      "  Pass [3] throughput: 127 infer/sec. Avg latency: 55188 usec (std 235 usec)\n",
      "  Client: \n",
      "    Request count: 381\n",
      "    Throughput: 127 infer/sec\n",
      "    Avg latency: 55188 usec (standard deviation 235 usec)\n",
      "    p50 latency: 55114 usec\n",
      "    p90 latency: 55552 usec\n",
      "    p95 latency: 55661 usec\n",
      "    p99 latency: 56117 usec\n",
      "    Avg HTTP time: 55199 usec (send 7 usec + response wait 55191 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 458\n",
      "    Execution count: 131\n",
      "    Successful request count: 131\n",
      "    Avg request latency: 54678 usec (overhead 5 usec + queue 27259 usec + compute input 15 usec + compute infer 27377 usec + compute output 22 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 144 infer/sec. Avg latency: 55369 usec (std 417 usec)\n",
      "  Pass [2] throughput: 144 infer/sec. Avg latency: 55346 usec (std 314 usec)\n",
      "  Pass [3] throughput: 144 infer/sec. Avg latency: 55369 usec (std 259 usec)\n",
      "  Client: \n",
      "    Request count: 432\n",
      "    Throughput: 144 infer/sec\n",
      "    Avg latency: 55369 usec (standard deviation 259 usec)\n",
      "    p50 latency: 55349 usec\n",
      "    p90 latency: 55714 usec\n",
      "    p95 latency: 55791 usec\n",
      "    p99 latency: 56117 usec\n",
      "    Avg HTTP time: 55351 usec (send 7 usec + response wait 55343 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 520\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 54804 usec (overhead 4 usec + queue 27326 usec + compute input 16 usec + compute infer 27432 usec + compute output 26 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 144 infer/sec. Avg latency: 62319 usec (std 11939 usec)\n",
      "  Pass [2] throughput: 145.333 infer/sec. Avg latency: 62270 usec (std 11933 usec)\n",
      "  Pass [3] throughput: 145.333 infer/sec. Avg latency: 62315 usec (std 11933 usec)\n",
      "  Client: \n",
      "    Request count: 436\n",
      "    Throughput: 145.333 infer/sec\n",
      "    Avg latency: 62315 usec (standard deviation 11933 usec)\n",
      "    p50 latency: 55545 usec\n",
      "    p90 latency: 83053 usec\n",
      "    p95 latency: 83276 usec\n",
      "    p99 latency: 83575 usec\n",
      "    Avg HTTP time: 62330 usec (send 7 usec + response wait 62322 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 520\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 61780 usec (overhead 5 usec + queue 34277 usec + compute input 16 usec + compute infer 27456 usec + compute output 26 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 145.333 infer/sec. Avg latency: 69261 usec (std 13790 usec)\n",
      "  Pass [2] throughput: 145.333 infer/sec. Avg latency: 69262 usec (std 13791 usec)\n",
      "  Pass [3] throughput: 145.333 infer/sec. Avg latency: 69307 usec (std 13789 usec)\n",
      "  Client: \n",
      "    Request count: 436\n",
      "    Throughput: 145.333 infer/sec\n",
      "    Avg latency: 69307 usec (standard deviation 13789 usec)\n",
      "    p50 latency: 82498 usec\n",
      "    p90 latency: 83443 usec\n",
      "    p95 latency: 83722 usec\n",
      "    p99 latency: 84247 usec\n",
      "    Avg HTTP time: 69293 usec (send 8 usec + response wait 69284 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 520\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 68732 usec (overhead 4 usec + queue 41222 usec + compute input 18 usec + compute infer 27459 usec + compute output 29 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 36 infer/sec, latency 27763 usec\n",
      "Concurrency: 2, throughput: 36.6667 infer/sec, latency 54770 usec\n",
      "Concurrency: 3, throughput: 54.3333 infer/sec, latency 55077 usec\n",
      "Concurrency: 4, throughput: 72.3333 infer/sec, latency 55069 usec\n",
      "Concurrency: 5, throughput: 90 infer/sec, latency 55154 usec\n",
      "Concurrency: 6, throughput: 109.333 infer/sec, latency 55244 usec\n",
      "Concurrency: 7, throughput: 127 infer/sec, latency 55188 usec\n",
      "Concurrency: 8, throughput: 144 infer/sec, latency 55369 usec\n",
      "Concurrency: 9, throughput: 145.333 infer/sec, latency 62315 usec\n",
      "Concurrency: 10, throughput: 145.333 infer/sec, latency 69307 usec\n"
     ]
    }
   ],
   "source": [
    "modelName = \"bertQA-onnx-trt-dynbatch\"\n",
    "maxConcurency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \"+modelName)\n",
    "!bash ./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have observed a fairly dramatic improvement in both latency and throughput. \n",
    "* How big is the impact in comparison to vanilla ONNX configuration or vanilla TorchScript? \n",
    "* What do you think was bottlenecking the multiple instance implementation?\n",
    "\n",
    "Discuss the results with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Congratulations!</h3><br>\n",
    "You've leaned some strategies to improve the GPU utilization and reduce latency using:\n",
    "\n",
    "* Concurrent model execution\n",
    "* Scheduling\n",
    "* Dynamic batching\n",
    "\n",
    "In the next segment of the class we will make a more formal assessment of inference performance across multiple concurrency levels and how to analyze your inference performance in a structured way. Please proceed to the next notebook:<br>\n",
    "[3.0 Server Performance](030_ServerPerformance.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
