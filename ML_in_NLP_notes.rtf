{\rtf1\ansi\ansicpg1252\cocoartf2639
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red255\green255\blue255;\red51\green51\blue51;}
{\*\expandedcolortbl;;\cssrgb\c100000\c100000\c100000;\cssrgb\c26275\c26275\c26275;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs28 \cf0 \cb2 \expnd0\expndtw0\kerning0
BUILDING TRANSFORMER-BASED NLP APPLICATIONS\
\
\
To join the course platform (available until 6 months after the course on 21 March 2022):\cb1 \
\cb2 1: Log in at https://courses.nvidia.com/join\cb1 \
\cb2 2: Navigate to https://courses.nvidia.com/dli-event\cb1 \
\cb2 3: Enter event code  GTC21M_NLP_BH89 \cb1 \
\pard\pardeftab720\partightenfactor0
\cf0 \cb2 \ul \ulc0 \
Progress\
\pard\pardeftab720\partightenfactor0
\cf0 \ulnone > Lab 1\
>> 000 DONE (just run it)\
>> 010 DONE (just run it)\
>> 020 DONE reading! (see \'93lab_1_020.png\'94)\
>> 030 DONE\
> Lab 2\
>> 000 DONE\
>> 010 DONE\
>> 020 DONE\
>> 030 DONE\
> Lab 3\
>> 000 DONE\
>> 010 DONE\
>> 020 DONE\
>> 030 DONE\
>> 040 DONE\ul \
\
\ulnone # Get certificate:\
> Go to https://courses.nvidia.com/courses/course-v1:DLI+C-FX-03+V3/course\
> Assessment Coding Project\
> Click \'93LAUNCH\'94 => code the coding project in a new tab\
> Come back to the tab in which start was clicked => click \'93ASSESS TASK\'94\
\pard\pardeftab720\partightenfactor0
\cf0 \ul \
\pard\pardeftab720\partightenfactor0
\cf3 \ulnone You\'a0have\'a0to\'a0create\'a0an\'a0account\'a0{\field{\*\fldinst{HYPERLINK "https://courses.nvidia.com/dli-event/"}}{\fldrslt \cf0 https://courses.nvidia.com/dli-event/}}\cf0 \ul \
\pard\pardeftab720\partightenfactor0
\cf0 \ulnone \
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://courses.nvidia.com/dli-event/"}}{\fldrslt \cf0 https://courses.nvidia.com/dli-event/}}\
\
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=U9Zh57dGsH4"}}{\fldrslt \cf0 https://www.youtube.com/watch?v=U9Zh57dGsH4}}\ul \
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/"}}{\fldrslt \cf0 \ulnone https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/}}\
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=ghEmQSxT6tw"}}{\fldrslt \cf0 https://www.youtube.com/watch?v=ghEmQSxT6tw}}\
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/"}}{\fldrslt \cf3 \ulnone https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/}}\cf3 \ulnone \
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://link.springer.com/article/10.1007/s11042-020-10183-2"}}{\fldrslt \cf0 https://link.springer.com/article/10.1007/s11042-020-10183-2}}\
\pard\pardeftab720\partightenfactor0
\cf3 \cb1 \
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/dli/containers/dli-nlp-nemo"}}{\fldrslt \cf0 \cb2 https://catalog.ngc.nvidia.com/orgs/nvidia/teams/dli/containers/dli-nlp-nemo}}\cf0 \cb2 \
\pard\pardeftab720\partightenfactor0
\cf0 \ul \
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://github.com/NVIDIA/Megatron-LM"}}{\fldrslt \cf0 \ulnone https://github.com/NVIDIA/Megatron-LM}}\ulnone \
\
\pard\pardeftab720\partightenfactor0
\cf0 \
####################################\
####################################\
####################################\
\
%%time\
# The training takes about 2 minutes to run\
\
TC_DIR = "/dli/task/nemo/examples/nlp/text_classification"\
\
# set the values we want to override\
NUM_CLASSES = 3\
MAX_SEQ_LENGTH = 128\
PATH_TO_TRAIN_FILE = "/dli/task/data/NCBI_tc-3/train_nemo_format.tsv"\
PATH_TO_VAL_FILE = "/dli/task/data/NCBI_tc-3/dev_nemo_format.tsv"\
PATH_TO_TEST_FILE = "/dli/task/data/NCBI_tc-3/test_nemo_format.tsv"\
# disease domain inference sample answers should be 0, 1, 2 \
INFER_SAMPLES_0 = "In contrast no mutations were detected in the p53 gene suggesting that this tumour suppressor is not frequently altered in this leukaemia "\
INFER_SAMPLES_1 = "The first predictive testing for Huntington disease  was based on analysis of linked polymorphic DNA markers to estimate the likelihood of inheriting the mutation for HD"\
INFER_SAMPLES_2 = "Further studies suggested that low dilutions of C5D serum contain a factor or factors interfering at some step in the hemolytic assay of C5 rather than a true C5 inhibitor or inactivator"\
MAX_EPOCHS = 5\
AMP_LEVEL = 'O1'\
PRECISION = 16\
LR = 5.0e-05\
\
# Override the config values in the command line\
# FIXED\
!python $TC_DIR/text_classification_with_bert.py \\\
        model.dataset.num_classes=$NUM_CLASSES \\\
        model.dataset.max_seq_length=$MAX_SEQ_LENGTH \\\
        model.train_ds.file_path=$PATH_TO_TRAIN_FILE \\\
        model.validation_ds.file_path=$PATH_TO_VAL_FILE \\\
        model.test_ds.file_path=$PATH_TO_TEST_FILE \\\
        model.infer_samples=["$INFER_SAMPLES_0","$INFER_SAMPLES_1","$INFER_SAMPLES_2"] \\\
        model.optim.lr=$LR \\\
        trainer.max_epochs=$MAX_EPOCHS \\\
        trainer.amp_level=$AMP_LEVEL \\\
        trainer.precision=$PRECISION\
\
####################################\
####################################\
####################################\
\
%%time\
# The training takes about 2 minutes to run\
\
TC_DIR = "/dli/task/nemo/examples/nlp/text_classification"\
\
# set the values we want to override\
NUM_CLASSES = 3\
MAX_SEQ_LENGTH = 128\
PATH_TO_TRAIN_FILE = "/dli/task/data/NCBI_tc-3/train_nemo_format.tsv"\
PATH_TO_VAL_FILE = "/dli/task/data/NCBI_tc-3/dev_nemo_format.tsv"\
PATH_TO_TEST_FILE = "/dli/task/data/NCBI_tc-3/test_nemo_format.tsv"\
# disease domain inference sample answers should be 0, 1, 2 \
INFER_SAMPLES_0 = "In contrast no mutations were detected in the p53 gene suggesting that this tumour suppressor is not frequently altered in this leukaemia "\
INFER_SAMPLES_1 = "The first predictive testing for Huntington disease  was based on analysis of linked polymorphic DNA markers to estimate the likelihood of inheriting the mutation for HD"\
INFER_SAMPLES_2 = "Further studies suggested that low dilutions of C5D serum contain a factor or factors interfering at some step in the hemolytic assay of C5 rather than a true C5 inhibitor or inactivator"\
MAX_EPOCHS = 3\
AMP_LEVEL = 'O1'\
PRECISION = 16\
LR = 5.0e-05\
PRETRAINED_MODEL = 'megatron-bert-345m-cased'\
BATCH_SIZE = 8\
\
# Override the config values in the command line\
# Trying my own experiment with a different language model!\
!python $TC_DIR/text_classification_with_bert.py \\\
        model.dataset.num_classes=$NUM_CLASSES \\\
        model.dataset.max_seq_length=$MAX_SEQ_LENGTH \\\
        model.train_ds.file_path=$PATH_TO_TRAIN_FILE \\\
        model.validation_ds.file_path=$PATH_TO_VAL_FILE \\\
        model.test_ds.file_path=$PATH_TO_TEST_FILE \\\
        model.infer_samples=["$INFER_SAMPLES_0","$INFER_SAMPLES_1","$INFER_SAMPLES_2"] \\\
        model.optim.lr=$LR \\\
        model.train_ds.batch_size=$BATCH_SIZE\\\
        trainer.max_epochs=$MAX_EPOCHS \\\
        trainer.amp_level=$AMP_LEVEL \\\
        trainer.precision=$PRECISION \\\
        model.language_model.pretrained_model_name=$PRETRAINED_MODEL\
\
####################################\
####################################\
####################################\
\
# Running inference over the my_queries list\
model.classifytext(queries=my_queries, batch_size=64, max_seq_length=128)}